# Default values for haproxy-template-ic.

# Replica count for controller
# Default: 2 replicas for high availability with leader election
# Only the leader replica deploys configs; followers remain ready for failover
replicaCount: 2

# Controller image configuration
image:
  repository: ghcr.io/phihos/haproxy-template-ic
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion
  tag: ""

imagePullSecrets: [ ]
nameOverride: ""
fullnameOverride: ""

# Controller configuration
controller:
  # Name of the HAProxyTemplateConfig CRD
  # This is the name of the HAProxyTemplateConfig custom resource
  crdName: haproxy-template-ic-config

  # Debug HTTP server port (0 to disable)
  # Exposes /debug/vars and /debug/pprof endpoints for introspection
  # Access via: kubectl port-forward pod/controller-xxx 6060:6060
  debugPort: 0

  # Template libraries
  # Control which predefined template libraries to include
  # Libraries are merged in order: base -> ingress -> gateway -> haproxytech -> values.yaml
  # User-provided config in controller.config always takes precedence
  templateLibraries:
    # Base library: Resource-agnostic core configuration
    # Provides generic HAProxy configuration, error pages, and plugin orchestration
    # Uses resource_* patterns to discover and include resource-specific implementations
    base:
      enabled: true
    # Ingress library: Kubernetes Ingress resource support
    # Provides routing and backend management for networking.k8s.io/v1 Ingress resources
    # Implements resource_ingress_* plugin interface (backends, maps, routing logic)
    ingress:
      enabled: true
    # Gateway library: Kubernetes Gateway API support (placeholder for future implementation)
    # Will provide routing for gateway.networking.k8s.io resources (HTTPRoute, TCPRoute, etc.)
    # Will implement resource_gateway_* plugin interface
    gateway:
      enabled: false
    # HAProxyTech library: Support for haproxy.org/* annotations
    # Currently includes basic authentication (haproxy.org/auth-type, haproxy.org/auth-secret)
    # Works with both Ingress and Gateway API resources
    haproxytech:
      enabled: true

  # HAProxyTemplateConfig specification
  # This configuration is rendered into a HAProxyTemplateConfig CRD
  # Users can customize this configuration using Helm values or create
  # the HAProxyTemplateConfig resource manually
  config:
    # Reference to the Secret containing HAProxy Dataplane API credentials
    # The Secret is automatically created by the Helm chart
    credentialsSecretRef:
      # Name will be set by the template to: {{ include "haproxy-template-ic.fullname" . }}-credentials
      name: ""  # Will be filled in by Helm template
      # Namespace defaults to Release.Namespace if not specified

    podSelector:
      matchLabels:
        app: haproxy
        component: loadbalancer

    controller:
      healthzPort: 8080
      metricsPort: 9090

      # Leader election for high availability
      # When multiple controller replicas are running, only the leader performs
      # write operations (deploying configs to HAProxy). All replicas continue
      # watching resources, rendering configs, and validating (hot standby).
      leaderElection:
        # Enable leader election (recommended when replicaCount > 1)
        enabled: true
        # Name of the Lease resource used for coordination
        # Defaults to the Helm release fullname if not specified
        # This ensures multiple releases in the same namespace don't conflict
        leaseName: ""
        # Duration that non-leader candidates wait to acquire leadership
        # Default: 60s (failover happens within this time after leader failure)
        leaseDuration: 60s
        # Duration the leader retries refreshing leadership before giving up
        # Default: 15s (should be < lease_duration, typically 1/4 of it)
        renewDeadline: 15s
        # Interval between leadership refresh attempts
        # Default: 5s (should be < renew_deadline, typically 1/3 of it)
        retryPeriod: 5s

    dataplane:
      port: 5555
      # Minimum time between consecutive deployments (rate limiting)
      # Prevents rapid-fire deployments from hammering HAProxy instances
      # Default: 2s
      minDeploymentInterval: 2s
      # Interval for periodic drift prevention deployments
      # Triggers deployment if no deployment occurred within this interval
      # Helps detect and correct configuration drift from external changes
      # Default: 60s
      driftPreventionInterval: 60s

      # Directory paths for HAProxy auxiliary files
      # These paths are used for both validation and deployment
      # See: https://www.haproxy.com/documentation/haproxy-data-plane-api/reference/configuration-file/#resources
      mapsDir: /etc/haproxy/maps
      sslCertsDir: /etc/haproxy/certs
      generalStorageDir: /etc/haproxy/general
      configFile: /etc/haproxy/haproxy.cfg

    logging:
      verbose: 1  # 0=WARNING, 1=INFO, 2=DEBUG

    watchedResourcesIgnoreFields:
      - metadata.managedFields

    watchedResources:
      ingresses:
        apiVersion: networking.k8s.io/v1
        resources: ingresses
        indexBy: [ "metadata.namespace", "metadata.name" ]
        enableValidationWebhook: true
      services:
        apiVersion: v1
        resources: services
        indexBy: [ "metadata.namespace", "metadata.name" ]
      endpoints:
        apiVersion: discovery.k8s.io/v1
        resources: endpointslices
        indexBy: [ "metadata.labels.kubernetes\\.io/service-name" ]
      secrets:
        apiVersion: v1
        resources: secrets
        store: on-demand
        indexBy: [ "metadata.namespace", "metadata.name" ]

    # Template customization
    # The sections below (templateSnippets, maps, files, haproxyConfig, validationTests)
    # are provided by template libraries (see controller.templateLibraries above).
    # You can override or extend library templates by defining them here.
    # User-provided values always take precedence over library defaults.
    #
    # Example - Override a specific template snippet:
    # templateSnippets:
    #   backend-name:
    #     template: >-
    #       custom_{{ ingress.metadata.name }}
    #
    # Example - Add a custom map:
    # maps:
    #   custom.map:
    #     template: |
    #       # Your custom map content
    #
    # Example - Add custom validation tests:
    # validationTests:
    #   test-custom-feature:
    #     description: Test custom feature
    #     fixtures:
    #       ingresses: [...]
    #     assertions: [...]

    # Uncomment and customize as needed:
    # templateSnippets:
    # maps:
    # files:
    # haproxyConfig:
    # validationTests:

# Webhook configuration
# Kubernetes admission webhook for resource validation
# ValidatingWebhookConfiguration is created by Helm at installation time
webhook:
  # Enable webhook validation
  # Requires watched_resources to have enable_validation_webhook: true
  enabled: true

  # Webhook HTTPS server port (inside container)
  port: 9443

  # Secret name containing webhook TLS certificates
  # The Secret must contain keys: tls.crt, tls.key, ca.crt
  # Defaults to: {{ include "haproxy-template-ic.fullname" . }}-webhook-cert
  # Leave empty to use the default naming convention
  secretName: ""

  # Service configuration for webhook endpoint
  service:
    # Service port (exposed to Kubernetes API server)
    port: 443

  # Certificate management
  # Choose ONE of the following options:
  #
  # Option 1: Use cert-manager (recommended for production)
  # Requires cert-manager to be installed in the cluster
  certManager:
    enabled: false
    # Issuer reference for cert-manager Certificate resource
    issuerRef:
      name: selfsigned-issuer
      kind: Issuer
      # group: cert-manager.io  # Optional, defaults to cert-manager.io
    # Certificate validity duration (default: 1 year)
    duration: 8760h  # 1 year
    # Renew certificate when this much time is left (default: 30 days)
    renewBefore: 720h  # 30 days

  # Option 2: Manual certificate management
  # Provide base64-encoded CA bundle for ValidatingWebhookConfiguration
  # This is required when certManager.enabled is false
  # Create the certificate Secret manually with keys: tls.crt, tls.key, ca.crt
  # Secret name is configured via webhook.secretName (default: haproxy-webhook-certs)
  caBundle: ""  # Base64-encoded CA certificate
  # Example:
  # caBundle: LS0tLS1CRUdJTi...

# Dataplane API credentials
credentials:
  dataplane:
    username: admin
    password: adminpass

# ServiceAccount configuration
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials
  automount: true
  # Annotations to add to the service account
  annotations: { }
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# RBAC configuration
rbac:
  # Specifies whether RBAC resources should be created
  create: true

# Pod annotations
podAnnotations: { }

# Pod labels
podLabels: { }

# Pod security context
podSecurityContext: { }
# fsGroup: 2000

# Container security context
securityContext: { }
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
# runAsUser: 1000

# Service configuration
# Exposes controller endpoints (health and metrics)
service:
  type: ClusterIP
  # Health check endpoint port
  healthzPort: 8080
  # Prometheus metrics endpoint port
  # Must match controller.config.controller.metrics_port
  metricsPort: 9090

# Liveness and readiness probes
# Disabled until /healthz endpoint is implemented in the controller
# livenessProbe:
#   httpGet:
#     path: /healthz
#     port: healthz
#   initialDelaySeconds: 10
#   periodSeconds: 10
#
# readinessProbe:
#   httpGet:
#     path: /healthz
#     port: healthz
#   initialDelaySeconds: 5
#   periodSeconds: 5

# Resources limits and requests
# Note: The controller automatically detects and respects container resource limits:
# - CPU limits: Go 1.25+ automatically sets GOMAXPROCS based on cgroup CPU limits
# - Memory limits: automemlimit library automatically sets GOMEMLIMIT to 90% of cgroup memory limits
#
# The AUTOMEMLIMIT environment variable can be used to adjust the memory limit ratio (default: 0.9)
# Example: AUTOMEMLIMIT=0.8 sets GOMEMLIMIT to 80% of the container memory limit
resources: { }
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 200m
  #   memory: 256Mi
  # requests:
  #   cpu: 100m
#   memory: 128Mi

# Node selector
nodeSelector: { }

# Tolerations
tolerations: [ ]

# Affinity rules
affinity: { }

# Autoscaling configuration
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Pod Disruption Budget
podDisruptionBudget:
  enabled: false
  # Only one of minAvailable or maxUnavailable should be set
  minAvailable: 1
  # maxUnavailable: 1

# Prometheus Metrics and Monitoring
# The controller exposes 11 Prometheus metrics on port 9090 covering:
# - Reconciliation cycles and errors
# - Deployment operations and duration
# - Configuration validation
# - Resource counts and event bus activity
#
# Metrics endpoint: http://<pod-ip>:9090/metrics
# Access via: kubectl port-forward pod/<controller-pod> 9090:9090
#
# For complete metric definitions and queries, see:
# pkg/controller/metrics/README.md in the repository
monitoring:
  # ServiceMonitor for Prometheus Operator
  # Creates a ServiceMonitor resource that configures Prometheus to scrape metrics
  serviceMonitor:
    enabled: false
    # Scrape interval for metrics collection
    interval: 30s
    # Timeout for each scrape
    scrapeTimeout: 10s
    # Additional labels for ServiceMonitor resource
    # Used by Prometheus to select which ServiceMonitors to use
    labels: { }
    #   prometheus: kube-prometheus
    #   release: prometheus
    # Relabeling configurations applied before scraping
    # https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config
    relabelings: [ ]
    # Example: Add cluster label
    # - sourceLabels: [__address__]
    #   targetLabel: cluster
    #   replacement: production
    # Metric relabeling configurations applied to scraped metrics
    metricRelabelings: [ ]
    # Example: Drop specific metrics
    # - sourceLabels: [__name__]
    #   regex: 'haproxy_ic_event_subscribers'
    #   action: drop

# NetworkPolicy configuration
networkPolicy:
  enabled: true

  # Egress rules
  egress:
    # Allow DNS resolution (required for service discovery)
    allowDNS: true

    # Kubernetes API Server access (required for watching resources)
    # Configure based on your cluster setup
    kubernetesApi:
      # For kind/standard clusters
      # Adjust this CIDR based on your cluster
      - cidr: 0.0.0.0/0  # Allow all by default, restrict in production
        ports:
          - port: 443
            protocol: TCP
          - port: 6443
            protocol: TCP

    # HAProxy Dataplane API pods (in any namespace)
    haproxyPods:
      # Allow access to all pods matching the pod_selector
      enabled: true
      # Pod selector for HAProxy pods
      podSelector:
        matchLabels:
          app: haproxy
          component: loadbalancer
      # Namespace selector - empty {} means all namespaces
      namespaceSelector: { }
      ports:
        - port: 5555  # Dataplane API
          protocol: TCP
        - port: 8404  # Health/stats
          protocol: TCP

    # Additional custom egress rules
    additionalRules: [ ]
    # Example:
    # - to:
    #     - namespaceSelector:
    #         matchLabels:
    #           name: monitoring
    #   ports:
    #     - port: 9090
    #       protocol: TCP

  # Ingress rules
  ingress:
    # Allow Prometheus/monitoring to scrape metrics
    # IMPORTANT: Enable this if using ServiceMonitor with NetworkPolicy
    # Without this, Prometheus cannot access the metrics endpoint
    monitoring:
      enabled: false  # Set to true when using Prometheus with NetworkPolicy
      # Pod selector for monitoring systems (e.g., Prometheus)
      # Match the labels of your Prometheus pods
      podSelector: { }
      #   matchLabels:
      #     app: prometheus
      #     app.kubernetes.io/name: prometheus
      # Namespace selector for monitoring systems
      # Match the namespace where Prometheus is running
      namespaceSelector: { }
      #   matchLabels:
      #     name: monitoring
      #     kubernetes.io/metadata.name: monitoring
      ports:
        - port: 9090  # Metrics endpoint
          protocol: TCP

    # Allow health checks from load balancers/ingress
    healthChecks:
      enabled: true
      # Allow from anywhere for health checks
      from:
        - podSelector: { }
      ports:
        - port: 8080  # Healthz
          protocol: TCP

    # Allow Kubernetes API server to call webhook
    # IMPORTANT: Enable this when webhook is enabled with NetworkPolicy
    webhook:
      enabled: true  # Automatically enabled when webhook.enabled is true
      # Allow from all sources (API server nodes)
      # The Kubernetes API server needs to reach the webhook endpoint
      from:
        - podSelector: { }
        - namespaceSelector: { }

    # Additional custom ingress rules
    additionalRules: [ ]
